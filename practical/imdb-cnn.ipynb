{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Neural Networks for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Movie sentiment dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates the use of Convolution1D for text classification - taken from Keras github source.\n",
    "Original code claims to get 0.89 test accuracy after 2 epochs.\n",
    "90s/epoch on Intel i5 2.4Ghz CPU.\n",
    "10s/epoch on Tesla K40 GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the following code, you may get various test accuracy (can be also lower than original claim of this code). \n",
    "\n",
    "Try to investigate the performance by changing arbitrarily parameters:\n",
    "\n",
    "* epochs\n",
    "* maxlen\n",
    "* max_features\n",
    "* kernel_size\n",
    "* batch_size\n",
    "\n",
    "and also hyperparameters:\n",
    "\n",
    "* comment dropout layer\n",
    "* change non-linear function in activation layer\n",
    "\n",
    "And discuss why these parameters influence the accuracy performance of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import _pickle as cPickle\n",
    "\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingHistory(Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # loss per batch\n",
    "        self.loss = []\n",
    "        self.acc = []\n",
    "        # loss per epoch\n",
    "        self.ep_loss = []\n",
    "        self.ep_acc = []\n",
    "        self.ep_val_loss = []\n",
    "        self.ep_val_acc = []       \n",
    "        self.i = 0\n",
    "        self.i_ep = 0\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.ep_loss.append(logs.get('loss'))\n",
    "        self.ep_acc.append(logs.get('acc'))\n",
    "        self.ep_val_loss.append(logs.get('val_loss'))\n",
    "        self.ep_val_acc.append(logs.get('val_loss'))\n",
    "        self.i_ep += 1\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = TrainingHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB Movie reviews sentiment classification\n",
    "\n",
    "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns 2 tuples:\n",
    "\n",
    "* x_train, x_test: list of sequences, which are lists of indexes (integers). If the num_words argument was specific, the maximum possible index value is num_words-1. If the maxlen argument was specified, the largest possible sequence length is maxlen.\n",
    "* y_train, y_test: list of integer labels (1 or 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class label 0 is `negative` sentiment, while 1 is `positive` sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look-up dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('asshats', 62855),\n",
       " ('death', 338),\n",
       " ('jesues', 68543),\n",
       " ('scandalously', 73137),\n",
       " ('nyman', 38432)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_index.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_word_index= dict((v,k) for (k,v) in word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'the'), (2, 'and'), (3, 'a'), (4, 'of'), (5, 'to')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed_word_index.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding integer sequences to fixed length vector\n",
    "\n",
    "(we will feed this fixed length vector as input of our model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv-Nets model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen, name='embedding_layer'))\n",
    "model.add(Dropout(0.2, name='dropout_1'))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1, name='conv_layer'))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D(name='max_pooling'))\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims, name='dense_layer'))\n",
    "model.add(Dropout(0.2, name='dropout_2'))\n",
    "model.add(Activation('relu', name='relu_dense'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1, name='prediction_layer'))\n",
    "model.add(Activation('sigmoid', name='sigm_prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv_layer (Conv1D)          (None, 398, 250)          37750     \n",
      "_________________________________________________________________\n",
      "max_pooling (GlobalMaxPoolin (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "relu_dense (Activation)      (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "prediction_layer (Dense)     (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "sigm_prediction (Activation) (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 350,751\n",
      "Trainable params: 350,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time() # Start time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 861s 34ms/step - loss: 0.3308 - acc: 0.8564 - val_loss: 0.2655 - val_acc: 0.8914\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 870s 35ms/step - loss: 0.2133 - acc: 0.9168 - val_loss: 0.2625 - val_acc: 0.8900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f67c9312908>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[history]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1733.23 seconds\n"
     ]
    }
   ],
   "source": [
    "end = time.time() # End time\n",
    "elapsed = end - start\n",
    "print(\"Training time: %.2f seconds\" %elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "HISTORY_PATH = 'history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving file into pickle format\n",
    "def savePickle(dataToWrite,pickleFilename):\n",
    "    f = open(pickleFilename, 'wb')\n",
    "    cPickle.dump(dataToWrite, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading file in pickle format\n",
    "def readPickle(pickleFilename):\n",
    "    f = open(pickleFilename, 'rb')\n",
    "    obj = cPickle.load(f)\n",
    "    f.close()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "savePickle(history.loss, os.path.join(HISTORY_PATH,'imdb_cnn_batch_loss'))\n",
    "savePickle(history.acc, os.path.join(HISTORY_PATH,'imdb_cnn_batch_accuracy'))\n",
    "savePickle(history.ep_loss, os.path.join(HISTORY_PATH,'imdb_cnn_epoch_loss'))\n",
    "savePickle(history.ep_acc, os.path.join(HISTORY_PATH,'imdb_cnn_epoch_accuracy'))\n",
    "savePickle(history.ep_val_loss, os.path.join(HISTORY_PATH,'imdb_cnn_epoch_val_loss'))\n",
    "savePickle(history.ep_val_acc, os.path.join(HISTORY_PATH,'imdb_cnn_epoch_val_accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = readPickle(os.path.join(HISTORY_PATH,'imdb_cnn_batch_loss'))\n",
    "batch_accuracy  = readPickle(os.path.join(HISTORY_PATH,'imdb_cnn_batch_accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch_loss = [ i for i in range(len(batch_loss))]\n",
    "Y_batch_loss = batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch_accuracy = [ i for i in range(len(batch_accuracy))]\n",
    "Y_batch_accuracy = batch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = go.Scatter(\n",
    "\tx = X_batch_loss,\n",
    "\ty = Y_batch_loss,\n",
    "\tmode = 'lines'\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = go.Scatter(\n",
    "\tx = X_batch_accuracy,\n",
    "\ty = Y_batch_accuracy,\n",
    "\tmode = 'lines'\n",
    "\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loss = [loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_acc = [acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_loss = go.Layout(\n",
    "\t\ttitle = 'Training error loss',\n",
    "\t\txaxis = dict(\n",
    "\t\t\t \ttitle ='Batch-epoch'\n",
    "\t\t\t),\n",
    "\t\tyaxis = dict(\n",
    "\t\t\t\ttitle = 'Error'\n",
    "\t\t\t)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_acc = go.Layout(\n",
    "\t\ttitle = 'Training accuracy',\n",
    "\t\txaxis = dict(\n",
    "\t\t\t \ttitle ='Batch-epoch'\n",
    "\t\t\t),\n",
    "\t\tyaxis = dict(\n",
    "\t\t\t\ttitle = 'Accuracy'\n",
    "\t\t\t)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_loss = go.Figure(data=data_loss, layout=layout_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///media/sf_Git/Deep-Learning-for-NLP-Dev/workshop/history/imdb_cnn_batch_loss.html'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you want to save image file (offline version), un-comment this\n",
    "# plot(fig_loss, filename= os.path.join(HISTORY_PATH,'imdb_cnn_batch_loss.html'), image='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~iftitahu/151.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "py.iplot(fig_loss, filename='imdb_cnn_batch_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](history/imdb_cnn_batch_loss.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_acc = go.Figure(data=data_acc, layout=layout_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///media/sf_Git/Deep-Learning-for-NLP-Dev/workshop/history/imdb_cnn_batch_accuracy.html'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you want to save image file (offline version), un-comment this\n",
    "# plot(fig_acc, filename= os.path.join(HISTORY_PATH,'imdb_cnn_batch_accuracy.html'), image='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~iftitahu/149.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "py.iplot(fig_acc, filename='imdb_cnn_batch_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](history/imdb_cnn_batch_accuracy.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 96s 4ms/step\n",
      "Test score: 0.262518066916\n",
      "Test accuracy: 0.89004\n"
     ]
    }
   ],
   "source": [
    "error_score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', error_score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
